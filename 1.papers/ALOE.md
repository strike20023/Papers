---
link: https://arxiv.org/pdf/2410.03642
github: https://github.com/ShujinWu-0814/ALOE
note: 通过“交互式对齐”让LLM在多轮对话中隐式推断当前用户的未明说个体偏好，并动态调整后续行为与回答；构建3310个用户人格、3K+树状多轮偏好对话数据，结合SFT与RL训练；提出 ALOE 基准进行评测，已录用 COLING 2025。
title: Aligning LLMs with Individual Preferences via Interaction
title_cn: 通过交互对齐大语言模型到个体偏好
tags:
  - paper
icon: LiNewspaper
---
## 摘要
随着LLM能力提升，除“有用、无害、诚实”等通用原则外，如何对齐到“个体化、多样化偏好”成为关键但被忽视的问题。本文提出“交互式对齐（interact to align）”的思路：让模型在多轮对话中隐式推断当前用户的未明说个体偏好，并据此动态对齐后续行为与响应。作者构建了 3,310 个多样化用户人格、基于多模型协作生成和筛选的 3K+ 多轮偏好数据（树结构），并以监督微调（SFT）与强化学习（RL）进行训练。为评估，提出 ALOE（ALign With CustOmized PrEferences）基准，包含 100 个精心设计的示例与指标以衡量对话过程中的个体化对齐性能。实验证明该方法能有效提升动态、个性化的偏好对齐能力。 （arXiv:2410.03642，COLING 2025 录用）

## 引言
- 目标：超越“一刀切”的通用对齐，关注个体用户的差异化偏好与期望，提升交互体验的定制化程度。
- 挑战：用户偏好往往隐性、动态、对话上下文相关；直接询问偏好可能不完整或不自然，需模型“读懂”对话信号并持续调整。
- 核心理念：通过多轮交互隐式推断个体偏好（语言风格、细节程度、结构化输出、保守或探索倾向等），并实时对齐响应策略。

## 方法
1) 多样化用户人格库（3,310 个）
- 先人工设计种子样例，再利用模型自生成与过滤迭代扩充，覆盖广泛的个体特征与偏好维度。

2) 多模型协作生成树状多轮偏好数据（3K+ 对话）
- 在给定人格指导下生成多轮对话，形成分支树结构以覆盖不同偏好轨迹；包含偏好显性表达与隐性暗示两类信号。
****
1) 训练流程：SFT + RL
- 先使用监督微调使模型学习基本的偏好识别与对齐模式，再通过强化学习优化在多轮交互中的动态适配能力。

4) 评测基准：ALOE（100 个示例 + 指标）
- 设计面向对话过程的定制化对齐评价方案，衡量模型在推断偏好、保持一致性与响应质量上的综合表现。

## 评测与结果
- 指标：围绕偏好推断准确性、对齐稳定性、对话质量等维度进行量化评测。
- 结论：模型在多轮对话中能更好地识别并适配个体偏好，显著提升定制化体验与响应一致性。
- 备注：论文已被 COLING 2025 接收。

## 贡献亮点
- 提出“交互式对齐”的范式：不再仅依赖静态指令或单轮信号，而是通过连续对话隐式推断偏好并动态对齐。
- 构建规模化的个体偏好资源：3,310 人格库与 3K+ 树状偏好对话，为个性化对齐提供数据基础。
- 训练与评测全链路：结合 SFT 与 RL 的训练流程，提出面向个体化对齐的 ALOE 基准与指标体系。

## 局限与讨论
- 人格与偏好覆盖度：尽管数量可观，仍可能存在特定人群或文化背景的偏好缺口。
- 隐式推断可靠性：需平衡过度拟合（迎合）与稳健性（不曲解）；推断错误可能导致体验反而下降。
- 强化学习奖励设计：如何准确刻画“对齐程度”与“对话质量”的权衡，仍有优化空间。
- 安全与伦理：个体化对齐过程中需严格约束安全、隐私与公平性，避免潜在滥用。

